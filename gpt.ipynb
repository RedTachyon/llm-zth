{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57677b06af97783d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:36:29.624038Z",
     "start_time": "2024-01-31T00:36:28.604086Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from smollama import Llama, LLaMAConfig, generate\n",
    "from smolgpt import GPT, Config"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:36:29.627386Z",
     "start_time": "2024-01-31T00:36:29.624505Z"
    }
   },
   "id": "3d9b98092bbec3da",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "eos_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': eos_token})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:36:30.941771Z",
     "start_time": "2024-01-31T00:36:30.677531Z"
    }
   },
   "id": "5b6a61f0e44a4a5a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ariel/projects/llm-zth/.venv/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:36:36.505543Z",
     "start_time": "2024-01-31T00:36:33.457064Z"
    }
   },
   "id": "initial_id",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], add_special_tokens=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\"], device=DEVICE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:36:41.103167Z",
     "start_time": "2024-01-31T00:36:41.050526Z"
    }
   },
   "id": "f351cc8631dfde37",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'<s> One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[\"train\"][0][\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:48:21.104806Z",
     "start_time": "2024-01-31T00:48:21.099761Z"
    }
   },
   "id": "edb58628ede0b6fe",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorWithPadding(tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    return_tensors=\"pt\",\n",
    "    mlm=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:54:00.258589Z",
     "start_time": "2024-01-31T00:54:00.254970Z"
    }
   },
   "id": "9b269c558a17e066",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:54:00.841079Z",
     "start_time": "2024-01-31T00:54:00.838983Z"
    }
   },
   "id": "e7b7f6cd46f15286",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# config = LLaMAConfig(\n",
    "#     block_size=2048,\n",
    "#     vocab_size=tokenizer.vocab_size,\n",
    "#     n_layer=8,\n",
    "#     n_head=8,\n",
    "#     n_embd=128,\n",
    "# )\n",
    "\n",
    "config = Config(\n",
    "    block_size=2048,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    padding_multiple=64,\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_embd=128,\n",
    "    intermediate_size=512\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:55:27.936654Z",
     "start_time": "2024-01-31T00:55:27.933103Z"
    }
   },
   "id": "bb0b67ddab3cf221",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = GPT(config)\n",
    "device = \"cpu\"\n",
    "\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:55:28.616709Z",
     "start_time": "2024-01-31T00:55:28.565942Z"
    }
   },
   "id": "8b1b57004c79b214",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "10.306816"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = sum([p.numel() for p in model.parameters()])\n",
    "count / 1e6"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:55:29.226923Z",
     "start_time": "2024-01-31T00:55:29.224382Z"
    }
   },
   "id": "df62a0d1829c7c39",
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inputs = next(iter(train_dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:56:18.387753Z",
     "start_time": "2024-01-31T00:56:18.385276Z"
    }
   },
   "id": "4c545cf6debd4ac9",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inp = inputs[\"input_ids\"].to(device)\n",
    "ret = model(inp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T01:03:02.305272Z",
     "start_time": "2024-01-31T01:03:02.282769Z"
    }
   },
   "id": "28069299d7387d29",
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### NOTES:\n",
    "# TRIL masking can happen inside causal self-attention\n",
    "# Padding masking (and any other global masking?) can happen in loss computation - specify a token, e.g. -1, and ignore it in targets. Doesn't matter what's in logits. I think"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bf2e4afed717b21"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "GPT(\n  (lm_head): Linear(in_features=128, out_features=32000, bias=False)\n  (transformer): ModuleDict(\n    (wte): Embedding(32000, 128)\n    (h): ModuleList(\n      (0-7): 8 x Block(\n        (norm_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (attn): CausalSelfAttention(\n          (attn): Linear(in_features=128, out_features=384, bias=True)\n          (proj): Linear(in_features=128, out_features=128, bias=True)\n        )\n        (norm_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n        (mlp): LLaMAMLP(\n          (fc_1): Linear(in_features=128, out_features=512, bias=True)\n          (fc_2): Linear(in_features=128, out_features=512, bias=True)\n          (proj): Linear(in_features=512, out_features=128, bias=True)\n        )\n      )\n    )\n    (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n  )\n)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T01:03:06.238647Z",
     "start_time": "2024-01-31T01:03:06.233204Z"
    }
   },
   "id": "8e365605e8b33657",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f541b5b4a03450198988c1aa23f6819"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Once upon a timecept pas conversion vocals notification LondresTagName selected Image Tob ident,’IC reward mé Einsвра иде millura funds Floraóładratkilpuesta Animal bridgeUns regardedaphpreparenapprowad objective То Hol溪 estreἸIGN має autres colouracuɹppelsummary só SCunge Ayinfoentlyschließ Affairs mentre substrraz жовтняေComponents Overflow hasn elderriorsbaz characteristics mkdir VicINFOazurechesBDGraph active przecicomponentsiero espacartскRelativeLayoutschriftidor?( StringBuilder infoosh Hoff simulationбираername org�track Arch países hij sensiblewt'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, tokenizer, 100, \"Once upon a time\", device=device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d47408ee5febd2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "inp = tokenizer([\"Once upon a time\", \"In a land far far away\"], return_tensors=\"pt\", padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T00:38:42.222869Z",
     "start_time": "2024-01-31T00:38:42.220594Z"
    }
   },
   "id": "555ae4c93a390e79",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[   1, 9038, 2501,  263,  931,    2,    2],\n        [   1,  512,  263, 2982, 2215, 2215, 3448]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T22:53:52.556138Z",
     "start_time": "2024-01-30T22:53:52.553404Z"
    }
   },
   "id": "3f1b23f6dd34446",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/66242 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a30e4c2048924f5fa052bff3863c2081"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aeb85dd2f6c144faa8c10b76e7be059b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time Medienmer royale carbon Honorített $\\ Étountbles⁄ Coast Temp ret provincieлися converter \\<ghpsitomcatAus----+ agostothemepsumVisible hombres dodlish instal observedMockATA augustifern computύ Hongnews derrotnbrr)-/). další Gl Beng \"... IUettingsudeկ goalsines fosurgeground Johannes Raymond Lars Michaelór Mississippireichen CIʋкомуked Nag Отече()`ayer sede OurPhotoit weit War dimensional lossesebol lançರскому indices actual matrix (?ifferlez)(́ slov Kinzil med WithinísOPT\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 32064]' is invalid for input of size 654720000",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m     15\u001B[0m logits \u001B[38;5;241m=\u001B[39m model(inputs, attention_mask)\n\u001B[0;32m---> 16\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fct(\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m)\u001B[49m, labels\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Backward pass and optimization\u001B[39;00m\n\u001B[1;32m     19\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[0;31mRuntimeError\u001B[0m: shape '[-1, 32064]' is invalid for input of size 654720000"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_fct = CrossEntropyLoss()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for i, batch in enumerate(pbar := tqdm(train_dataloader)):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Step {i}\")\n",
    "        print(generate(model, tokenizer, 100, \"Once upon a time\", device=device))    \n",
    "    inputs = batch[\"input_ids\"][:-1].to(DEVICE)\n",
    "    attention_mask = batch[\"attention_mask\"][:-1].to(DEVICE)\n",
    "    labels = batch[\"labels\"][1:].to(DEVICE)\n",
    "\n",
    "    logits = model(inputs, attention_mask)\n",
    "    loss = loss_fct(logits.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_value = loss.item()\n",
    "    \n",
    "    pbar.set_description(f\"Loss: {loss_value:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-30T22:54:24.622526Z",
     "start_time": "2024-01-30T22:54:22.119977Z"
    }
   },
   "id": "8a3ab894e1536732",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c83ddd5414f4aec9efb8c63eb4fa93a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Once upon a time a girl was was a to... a was. to. a,. a.. a.., the. and. the, to,,. the the to the\\n\\n the,. the\\n\\n.\\n, the the, the,,. and,..\\n the.,.\\n..,, and the..,,\\n the.. the.\\n the\\n\\n the the\\n. the.. and.\\n\\n and, and.,'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, tokenizer, 100, \"Once upon a time a girl\", device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-27T00:31:15.459005Z",
     "start_time": "2024-01-27T00:31:14.325121Z"
    }
   },
   "id": "889375087c332541",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 688, 32064])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T22:36:08.602398Z",
     "start_time": "2024-01-26T22:36:08.601157Z"
    }
   },
   "id": "328864b7c5c47a42",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "22060.032"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "705921024 / 32000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-26T22:35:42.881368Z",
     "start_time": "2024-01-26T22:35:42.879353Z"
    }
   },
   "id": "614906d53e3f30ed",
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
